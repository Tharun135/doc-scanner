# LlamaIndex + Ollama Configuration (Local AI - No API Key Required!)
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a model: ollama pull mistral OR ollama pull phi3
# 3. Start service: ollama serve
# 4. No API keys needed - everything runs locally!

# Optional: Specify preferred model (defaults to 'mistral')
OLLAMA_MODEL=mistral

# Optional: Ollama API endpoint (defaults to http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Legacy Google Gemini (deprecated - replaced by Ollama)
# GOOGLE_API_KEY=your_google_api_key_here
