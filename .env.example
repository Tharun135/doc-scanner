# Local AI Configuration
# For future use with local AI models like Ollama
# OLLAMA_URL=http://localhost:11434
